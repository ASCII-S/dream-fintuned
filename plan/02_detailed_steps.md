# Dream 7B + S1K å¾®è°ƒè¯¦ç»†å®æ–½æ­¥éª¤

---

## ğŸ”§ é˜¶æ®µ1ï¼šç¯å¢ƒå‡†å¤‡ä¸æ¶æ„åˆ†æ (Day 1, çº¦4-6å°æ—¶)

### æ­¥éª¤ 1.1ï¼šåˆ›å»ºPythonç¯å¢ƒ
```bash
# åˆ›å»ºè™šæ‹Ÿç¯å¢ƒ
conda create -n dream-sft python=3.10 -y
conda activate dream-sft

# æˆ–ä½¿ç”¨venv
python -m venv dream-env
source dream-env/bin/activate
```

### æ­¥éª¤ 1.2ï¼šå®‰è£…ä¾èµ–åŒ…
```bash
# æ ¸å¿ƒä¾èµ–
pip install torch==2.5.1 --index-url https://download.pytorch.org/whl/cu121
pip install transformers==4.46.2
pip install datasets
pip install accelerate
pip install sentencepiece
pip install protobuf

# è®­ç»ƒç›¸å…³ï¼ˆDreamä½¿ç”¨verlæ¡†æ¶ï¼‰
cd resp/Dream
pip install -e .

# ç›‘æ§ä¸å¯è§†åŒ–
pip install wandb
pip install tensorboard
pip install tqdm

# æ•°æ®å¤„ç†
pip install pandas
pip install pyarrow
pip install jsonlines
```

**å…³é”®ç‚¹**ï¼š
- Dreamè¦æ±‚ `torch==2.5.1` å’Œ `transformers==4.46.2`ï¼Œç‰ˆæœ¬å¿…é¡»åŒ¹é…
- ç¡®ä¿CUDAç‰ˆæœ¬å…¼å®¹ï¼ˆæ¨èCUDA 12.1+ï¼‰

### æ­¥éª¤ 1.3ï¼šéªŒè¯ç¯å¢ƒ
```python
# test_env.py
import torch
import transformers
from transformers import AutoModel, AutoTokenizer

print(f"PyTorchç‰ˆæœ¬: {torch.__version__}")
print(f"Transformersç‰ˆæœ¬: {transformers.__version__}")
print(f"CUDAå¯ç”¨: {torch.cuda.is_available()}")
print(f"GPUæ•°é‡: {torch.cuda.device_count()}")
if torch.cuda.is_available():
    print(f"GPUå‹å·: {torch.cuda.get_device_name(0)}")
    print(f"GPUæ˜¾å­˜: {torch.cuda.get_device_properties(0).total_memory / 1024**3:.2f} GB")
```

### æ­¥éª¤ 1.4ï¼šç†è§£Dreamæ‰©æ•£æ¨¡å‹æ¶æ„

**æ ¸å¿ƒæ¦‚å¿µ**ï¼š
1. **æ‰©æ•£è¿‡ç¨‹**ï¼šä¸åŒäºè‡ªå›å½’æ¨¡å‹çš„é€tokenç”Ÿæˆï¼ŒDreamä½¿ç”¨æ‰©æ•£è¿‡ç¨‹åŒæ—¶ç”Ÿæˆå¤šä¸ªtoken
2. **Remaskingç­–ç•¥**ï¼šæ§åˆ¶tokenç”Ÿæˆé¡ºåºçš„ç­–ç•¥
   - `origin`: éšæœºé¡ºåº
   - `entropy`: åŸºäºç†µçš„ç½®ä¿¡åº¦
   - `maskgit_plus`: top1ç½®ä¿¡åº¦
   - `topk_margin`: top1-top2è¾¹é™…ç½®ä¿¡åº¦

**å…³é”®æ–‡ä»¶é˜…è¯»**ï¼š
```bash
# é˜…è¯»è¿™äº›æ–‡ä»¶äº†è§£æ¶æ„
- resp/Dream/src/diffllm/modeling_diffllm.py  # æ¨¡å‹æ ¸å¿ƒä»£ç 
- resp/Dream/src/diffllm/configuration_diffllm.py  # é…ç½®æ–‡ä»¶
- resp/Dream/src/trainer/fsdp_sft_trainer.py  # è®­ç»ƒå™¨
```

**åŠ¨æ‰‹å®éªŒ**ï¼š
```python
# test_inference.py - æµ‹è¯•Dreamæ¨ç†
import torch
from transformers import AutoModel, AutoTokenizer

model_path = "Dream-org/Dream-v0-Instruct-7B"
model = AutoModel.from_pretrained(model_path, torch_dtype=torch.bfloat16, trust_remote_code=True)
tokenizer = AutoTokenizer.from_pretrained(model_path, trust_remote_code=True)
model = model.to("cuda").eval()

messages = [{"role": "user", "content": "è§£é‡Šä»€ä¹ˆæ˜¯æ‰©æ•£æ¨¡å‹"}]
inputs = tokenizer.apply_chat_template(messages, return_tensors="pt", return_dict=True, add_generation_prompt=True)
input_ids = inputs.input_ids.to("cuda")
attention_mask = inputs.attention_mask.to("cuda")

output = model.diffusion_generate(
    input_ids,
    attention_mask=attention_mask,
    max_new_tokens=256,
    steps=128,
    temperature=0.2,
    alg="entropy"
)

print(tokenizer.decode(output.sequences[0][len(input_ids[0]):].tolist()).split(tokenizer.eos_token)[0])
```

### æ­¥éª¤ 1.5ï¼šåˆ†æS1Kæ•°æ®é›†

```python
# explore_s1k.py - æ¢ç´¢S1Kæ•°æ®é›†
from datasets import load_dataset

# åŠ è½½æ•°æ®é›†
dataset = load_dataset("simplescaling/s1K-1.1", split="train")

print(f"æ•°æ®é›†å¤§å°: {len(dataset)}")
print(f"æ•°æ®å­—æ®µ: {dataset.column_names}")
print("\nç¬¬ä¸€ä¸ªæ ·æœ¬:")
print(dataset[0])

# ç»Ÿè®¡åˆ†æ
import pandas as pd
df = dataset.to_pandas()

# åˆ†ææ•°æ®æ¥æº
print("\næ•°æ®æ¥æºåˆ†å¸ƒ:")
print(df['source'].value_counts())

# åˆ†ææ–‡æœ¬é•¿åº¦
df['question_length'] = df['question'].apply(len)
df['answer_length'] = df['answer'].apply(len)
print(f"\né—®é¢˜å¹³å‡é•¿åº¦: {df['question_length'].mean():.0f} å­—ç¬¦")
print(f"ç­”æ¡ˆå¹³å‡é•¿åº¦: {df['answer_length'].mean():.0f} å­—ç¬¦")
```

**é¢„æœŸæ•°æ®æ ¼å¼**ï¼š
```json
{
  "question": "é—®é¢˜æ–‡æœ¬",
  "answer": "å¸¦æ¨ç†è¿‡ç¨‹çš„ç­”æ¡ˆ",
  "source": "æ•°æ®æ¥æºï¼ˆå¦‚AIME, GPQAç­‰ï¼‰",
  "thinking": "æ¨ç†è½¨è¿¹ï¼ˆå¯é€‰ï¼‰"
}
```

---

## ğŸ“Š é˜¶æ®µ2ï¼šæ•°æ®å‡†å¤‡ (Day 2, çº¦4-6å°æ—¶)

### æ­¥éª¤ 2.1ï¼šä¸‹è½½S1K-1.1æ•°æ®é›†

```python
# download_s1k.py
from datasets import load_dataset
import os

# ä¸‹è½½æ•°æ®é›†
dataset = load_dataset("simplescaling/s1K-1.1")
print(f"æ•°æ®é›†splits: {dataset.keys()}")

# ä¿å­˜åˆ°æœ¬åœ°
save_dir = "data/s1k"
os.makedirs(save_dir, exist_ok=True)
dataset.save_to_disk(save_dir)
print(f"æ•°æ®é›†å·²ä¿å­˜åˆ°: {save_dir}")
```

### æ­¥éª¤ 2.2ï¼šæ•°æ®æ ¼å¼è½¬æ¢

Dreamçš„SFTè®­ç»ƒéœ€è¦ä»¥ä¸‹æ ¼å¼ï¼š
- `prompt`: list of messagesï¼ˆé™¤äº†æœ€åä¸€æ¡assistantæ¶ˆæ¯ï¼‰
- `response`: strï¼ˆassistantçš„å›å¤å†…å®¹ï¼‰

```python
# prepare_data_for_dream.py
from datasets import load_dataset, Dataset
import json
import os

def convert_s1k_to_dream_format(sample):
    """
    å°†S1Kæ ¼å¼è½¬æ¢ä¸ºDreamè®­ç»ƒæ ¼å¼
    
    S1Kæ ¼å¼:
    {
        "question": "...",
        "answer": "...",  # å¯èƒ½åŒ…å«<think>...</think>æ ‡ç­¾
    }
    
    Dreamæ ¼å¼:
    {
        "prompt": [{"role": "user", "content": "..."}],
        "response": "..."
    }
    """
    question = sample.get('question', sample.get('prompt', ''))
    answer = sample.get('answer', sample.get('response', ''))
    
    # æ„é€ promptï¼ˆchatæ ¼å¼ï¼‰
    prompt = [
        {
            "role": "system",
            "content": "You are a helpful AI assistant. Think step by step and provide detailed reasoning."
        },
        {
            "role": "user", 
            "content": question
        }
    ]
    
    # responseå°±æ˜¯ç­”æ¡ˆï¼ˆåŒ…å«æ¨ç†è¿‡ç¨‹ï¼‰
    response = answer
    
    return {
        "prompt": prompt,
        "response": response
    }

# åŠ è½½S1Kæ•°æ®é›†
dataset = load_dataset("simplescaling/s1K-1.1", split="train")
print(f"åŸå§‹æ•°æ®é‡: {len(dataset)}")

# è½¬æ¢æ ¼å¼
converted_data = [convert_s1k_to_dream_format(sample) for sample in dataset]
converted_dataset = Dataset.from_list(converted_data)

# åˆ’åˆ†è®­ç»ƒé›†å’ŒéªŒè¯é›†ï¼ˆ90/10åˆ†å‰²ï¼‰
split_dataset = converted_dataset.train_test_split(test_size=0.1, seed=42)
train_dataset = split_dataset['train']
val_dataset = split_dataset['test']

print(f"è®­ç»ƒé›†å¤§å°: {len(train_dataset)}")
print(f"éªŒè¯é›†å¤§å°: {len(val_dataset)}")

# ä¿å­˜ä¸ºparquetæ ¼å¼ï¼ˆDreamè®­ç»ƒå™¨æ”¯æŒï¼‰
output_dir = "data/s1k_dream_format"
os.makedirs(output_dir, exist_ok=True)
train_dataset.to_parquet(f"{output_dir}/train.parquet")
val_dataset.to_parquet(f"{output_dir}/val.parquet")

print(f"æ•°æ®å·²ä¿å­˜åˆ°: {output_dir}")
print("\nç¤ºä¾‹æ•°æ®:")
print(json.dumps(train_dataset[0], indent=2, ensure_ascii=False))
```

### æ­¥éª¤ 2.3ï¼šæ•°æ®è´¨é‡æ£€æŸ¥

```python
# validate_data.py
import pandas as pd
from transformers import AutoTokenizer

# åŠ è½½æ•°æ®
train_df = pd.read_parquet("data/s1k_dream_format/train.parquet")
val_df = pd.read_parquet("data/s1k_dream_format/val.parquet")

# åˆå§‹åŒ–tokenizer
tokenizer = AutoTokenizer.from_pretrained("Dream-org/Dream-v0-Base-7B", trust_remote_code=True)

def check_length(row):
    """æ£€æŸ¥tokené•¿åº¦"""
    prompt = row['prompt']
    response = row['response']
    messages = prompt + [{"role": "assistant", "content": response}]
    tokens = tokenizer.apply_chat_template(messages, tokenize=True)
    return len(tokens)

# è®¡ç®—tokené•¿åº¦
train_df['token_length'] = train_df.apply(check_length, axis=1)

print("Tokené•¿åº¦ç»Ÿè®¡:")
print(train_df['token_length'].describe())
print(f"\nè¶…è¿‡2048 tokensçš„æ ·æœ¬æ•°: {(train_df['token_length'] > 2048).sum()}")
print(f"è¶…è¿‡1024 tokensçš„æ ·æœ¬æ•°: {(train_df['token_length'] > 1024).sum()}")

# è¿‡æ»¤è¿‡é•¿çš„æ ·æœ¬ï¼ˆå¯é€‰ï¼‰
max_length = 2048
filtered_train_df = train_df[train_df['token_length'] <= max_length]
print(f"\nè¿‡æ»¤åè®­ç»ƒé›†å¤§å°: {len(filtered_train_df)} (åŸå§‹: {len(train_df)})")

# ä¿å­˜è¿‡æ»¤åçš„æ•°æ®
if len(filtered_train_df) < len(train_df):
    filtered_train_df.drop(columns=['token_length']).to_parquet("data/s1k_dream_format/train_filtered.parquet")
```

---

## ğŸš€ é˜¶æ®µ3ï¼šå¾®è°ƒè®­ç»ƒ (Day 3-4, çº¦8-12å°æ—¶)

### æ­¥éª¤ 3.1ï¼šé…ç½®è®­ç»ƒè„šæœ¬

åˆ›å»ºè®­ç»ƒé…ç½®è„šæœ¬ï¼š

```bash
# scripts/run_sft_s1k.sh
#!/bin/bash

set -x

if [ "$#" -lt 2 ]; then
    echo "Usage: scripts/run_sft_s1k.sh <nproc_per_node> <save_path>"
    exit 1
fi

nproc_per_node=$1
save_path=$2

# æ•°æ®è·¯å¾„
DATA_DIR="data/s1k_dream_format"
TRAIN_FILE="$DATA_DIR/train.parquet"
VAL_FILE="$DATA_DIR/val.parquet"

# æ¨¡å‹è·¯å¾„
MODEL_PATH="Dream-org/Dream-v0-Base-7B"

# è®­ç»ƒå‚æ•°
LR=2e-6
BATCH_SIZE=4  # æ ¹æ®GPUæ˜¾å­˜è°ƒæ•´
MAX_LENGTH=2048
EPOCHS=3

# å¯åŠ¨è®­ç»ƒ
torchrun --standalone --nnodes=1 --nproc_per_node=$nproc_per_node \
    -m src.trainer.fsdp_sft_trainer \
    diffusion.time_reweighting=cart \
    data.train_files=$TRAIN_FILE \
    data.val_files=$VAL_FILE \
    data.max_length=$MAX_LENGTH \
    data.prompt_key=prompt \
    data.response_key=response \
    data.truncation=right \
    optim.lr=$LR \
    data.micro_batch_size_per_gpu=$BATCH_SIZE \
    data.enable_perbatch_cutoff=True \
    data.perbatch_cutoff_type=random_with_input_pad \
    +data.perbatch_cutoff=True \
    model.partial_pretrain=$MODEL_PATH \
    model.trust_remote_code=True \
    model.enable_gradient_checkpointing=True \
    trainer.default_local_dir=$save_path \
    trainer.project_name=dream-s1k-sft \
    trainer.experiment_name=s1k-$(date +%Y%m%d-%H%M%S) \
    trainer.logger=['console','wandb'] \
    trainer.total_epochs=$EPOCHS \
    trainer.save_freq=500
```

### æ­¥éª¤ 3.2ï¼šå¯åŠ¨è®­ç»ƒ

```bash
# å•GPUè®­ç»ƒ
cd resp/Dream
bash ../../scripts/run_sft_s1k.sh 1 ../../checkpoints/s1k-sft

# å¤šGPUè®­ç»ƒï¼ˆå¦‚æœæœ‰8å—GPUï¼‰
bash ../../scripts/run_sft_s1k.sh 8 ../../checkpoints/s1k-sft
```

**æ ¹æ®GPUæ˜¾å­˜è°ƒæ•´å‚æ•°**ï¼š

| GPUé…ç½® | batch_size | gradient_checkpointing | é¢„è®¡è®­ç»ƒæ—¶é—´ |
|---------|------------|------------------------|-------------|
| 1x 4090 (24GB) | 2 | True | ~8-10å°æ—¶ |
| 2x 4090 (48GB) | 4 | True | ~4-5å°æ—¶ |
| 1x A100 (40GB) | 4 | True | ~5-6å°æ—¶ |
| 8x GPU | 8 | False | ~1-2å°æ—¶ |

### æ­¥éª¤ 3.3ï¼šç›‘æ§è®­ç»ƒ

**ä½¿ç”¨WandBç›‘æ§**ï¼š
```bash
# ç™»å½•wandb
wandb login

# æŸ¥çœ‹è®­ç»ƒæ›²çº¿
# è®¿é—® https://wandb.ai/your-username/dream-s1k-sft
```

**ä½¿ç”¨TensorBoardç›‘æ§**ï¼š
```bash
# å¯åŠ¨tensorboard
tensorboard --logdir checkpoints/s1k-sft --port 6006

# åœ¨æµè§ˆå™¨è®¿é—® http://localhost:6006
```

**å‘½ä»¤è¡Œç›‘æ§**ï¼š
```bash
# ç›‘æ§GPUä½¿ç”¨
watch -n 1 nvidia-smi

# æŸ¥çœ‹è®­ç»ƒæ—¥å¿—
tail -f checkpoints/s1k-sft/train.log
```

### æ­¥éª¤ 3.4ï¼šå¤„ç†å¸¸è§é—®é¢˜

**é—®é¢˜1ï¼šOOM (Out of Memory)**
```bash
# è§£å†³æ–¹æ¡ˆï¼š
# 1. å‡å°batch_sizeï¼ˆä»4æ”¹åˆ°2æˆ–1ï¼‰
# 2. å¯ç”¨gradient_checkpointing
# 3. å‡å°max_lengthï¼ˆä»2048æ”¹åˆ°1024ï¼‰
# 4. ä½¿ç”¨æ›´å°‘çš„GPUè¿›ç¨‹
```

**é—®é¢˜2ï¼šè®­ç»ƒé€Ÿåº¦æ…¢**
```bash
# è§£å†³æ–¹æ¡ˆï¼š
# 1. ç¡®è®¤ä½¿ç”¨äº†bfloat16
# 2. æ£€æŸ¥æ•°æ®åŠ è½½æ˜¯å¦æ˜¯ç“¶é¢ˆ
# 3. å¢åŠ batch_sizeï¼ˆå¦‚æœæ˜¾å­˜å…è®¸ï¼‰
```

---

## ğŸ“ˆ é˜¶æ®µ4ï¼šè¯„ä¼°ä¸åˆ†æ (Day 4-5, çº¦4-6å°æ—¶)

### æ­¥éª¤ 4.1ï¼šåŠ è½½å¾®è°ƒåçš„æ¨¡å‹

```python
# load_finetuned_model.py
import torch
from transformers import AutoModel, AutoTokenizer

# åŠ è½½å¾®è°ƒåçš„checkpoint
checkpoint_path = "checkpoints/s1k-sft/checkpoint-final"  # æ ¹æ®å®é™…è·¯å¾„è°ƒæ•´
model = AutoModel.from_pretrained(checkpoint_path, torch_dtype=torch.bfloat16, trust_remote_code=True)
tokenizer = AutoTokenizer.from_pretrained("Dream-org/Dream-v0-Base-7B", trust_remote_code=True)

model = model.to("cuda").eval()
print("æ¨¡å‹åŠ è½½æˆåŠŸ!")
```

### æ­¥éª¤ 4.2ï¼šå®šæ€§è¯„ä¼°

```python
# qualitative_eval.py - å¯¹æ¯”æµ‹è¯•
import torch
from transformers import AutoModel, AutoTokenizer

def generate_response(model, tokenizer, prompt, max_tokens=512):
    messages = [{"role": "user", "content": prompt}]
    inputs = tokenizer.apply_chat_template(messages, return_tensors="pt", return_dict=True, add_generation_prompt=True)
    input_ids = inputs.input_ids.to("cuda")
    attention_mask = inputs.attention_mask.to("cuda")
    
    output = model.diffusion_generate(
        input_ids,
        attention_mask=attention_mask,
        max_new_tokens=max_tokens,
        steps=256,
        temperature=0.2,
        alg="entropy"
    )
    
    response = tokenizer.decode(output.sequences[0][len(input_ids[0]):].tolist())
    return response.split(tokenizer.eos_token)[0]

# åŠ è½½baseæ¨¡å‹å’Œå¾®è°ƒæ¨¡å‹
base_model = AutoModel.from_pretrained("Dream-org/Dream-v0-Base-7B", torch_dtype=torch.bfloat16, trust_remote_code=True).to("cuda").eval()
finetuned_model = AutoModel.from_pretrained("checkpoints/s1k-sft/checkpoint-final", torch_dtype=torch.bfloat16, trust_remote_code=True).to("cuda").eval()
tokenizer = AutoTokenizer.from_pretrained("Dream-org/Dream-v0-Base-7B", trust_remote_code=True)

# æµ‹è¯•é—®é¢˜
test_prompts = [
    "ä¸€ä¸ªæ•°çš„3å€åŠ ä¸Š5ç­‰äº26ï¼Œè¿™ä¸ªæ•°æ˜¯å¤šå°‘ï¼Ÿ",
    "è§£é‡Šä¸ºä»€ä¹ˆ2+2=4",
    "å†™ä¸€ä¸ªPythonå‡½æ•°æ¥è®¡ç®—æ–æ³¢é‚£å¥‘æ•°åˆ—çš„ç¬¬né¡¹"
]

for prompt in test_prompts:
    print(f"\n{'='*60}")
    print(f"é—®é¢˜: {prompt}")
    print(f"\n[Baseæ¨¡å‹å›ç­”]:")
    print(generate_response(base_model, tokenizer, prompt))
    print(f"\n[å¾®è°ƒåæ¨¡å‹å›ç­”]:")
    print(generate_response(finetuned_model, tokenizer, prompt))
```

### æ­¥éª¤ 4.3ï¼šå®šé‡è¯„ä¼°

```python
# quantitative_eval.py - åœ¨éªŒè¯é›†ä¸Šè¯„ä¼°
from datasets import load_dataset
from tqdm import tqdm
import json

# åŠ è½½éªŒè¯é›†
val_dataset = load_dataset("parquet", data_files="data/s1k_dream_format/val.parquet", split="train")

results = []
correct = 0

for sample in tqdm(val_dataset):
    question = sample['prompt'][-1]['content']
    ground_truth = sample['response']
    
    # ç”Ÿæˆç­”æ¡ˆ
    prediction = generate_response(finetuned_model, tokenizer, question)
    
    # ç®€å•çš„ç­”æ¡ˆåŒ¹é…ï¼ˆå®é™…éœ€è¦æ›´å¤æ‚çš„è¯„ä¼°é€»è¾‘ï¼‰
    is_correct = evaluate_answer(prediction, ground_truth)  # éœ€è¦å®ç°
    if is_correct:
        correct += 1
    
    results.append({
        "question": question,
        "ground_truth": ground_truth,
        "prediction": prediction,
        "correct": is_correct
    })

accuracy = correct / len(val_dataset)
print(f"\néªŒè¯é›†å‡†ç¡®ç‡: {accuracy:.2%}")

# ä¿å­˜ç»“æœ
with open("eval_results.json", "w") as f:
    json.dump(results, f, indent=2, ensure_ascii=False)
```

### æ­¥éª¤ 4.4ï¼šå‡†å¤‡å±•ç¤ºææ–™

åˆ›å»ºå±•ç¤ºæ–‡æ¡£ï¼š
```markdown
# Dream 7B + S1K å¾®è°ƒé¡¹ç›®æˆæœæŠ¥å‘Š

## 1. é¡¹ç›®æ¦‚è¿°
- ç›®æ ‡ï¼šä½¿ç”¨S1Kæ•°æ®é›†å¾®è°ƒDream 7Bæ¨¡å‹
- æ•°æ®ï¼š1000ä¸ªé«˜è´¨é‡æ¨ç†æ ·æœ¬
- è®­ç»ƒï¼š3 epochsï¼Œå­¦ä¹ ç‡2e-6

## 2. è®­ç»ƒè¿‡ç¨‹
- GPUé…ç½®ï¼š[å¡«å†™]
- è®­ç»ƒæ—¶é•¿ï¼š[å¡«å†™]
- æœ€ç»ˆLossï¼š[å¡«å†™]
- è®­ç»ƒæ›²çº¿ï¼š[æ’å…¥å›¾ç‰‡]

## 3. è¯„ä¼°ç»“æœ
### å®šé‡ç»“æœ
- éªŒè¯é›†å‡†ç¡®ç‡ï¼š[å¡«å†™]
- å¯¹æ¯”baseæ¨¡å‹æå‡ï¼š[å¡«å†™]

### å®šæ€§æ¡ˆä¾‹
[å±•ç¤º3-5ä¸ªå¯¹æ¯”æ¡ˆä¾‹]

## 4. æŠ€æœ¯æŒ‘æˆ˜ä¸è§£å†³
1. æŒ‘æˆ˜1ï¼š[æè¿°]
   - è§£å†³æ–¹æ¡ˆï¼š[æè¿°]
2. ...

## 5. æ¶æ„ç†è§£
- æ‰©æ•£æ¨¡å‹vsè‡ªå›å½’æ¨¡å‹çš„å·®å¼‚
- Remaskingç­–ç•¥çš„å½±å“
- ...

## 6. æœªæ¥æ”¹è¿›æ–¹å‘
- è¶…å‚æ•°ä¼˜åŒ–
- æ•°æ®å¢å¼º
- ...
```

---

## âœ… æ£€æŸ¥æ¸…å•

å®Œæˆä»¥ä¸‹æ‰€æœ‰ä»»åŠ¡åï¼Œæ‚¨å°±å¯ä»¥å‚åŠ ç»ˆé¢äº†ï¼š

- [ ] **ç¯å¢ƒé…ç½®**ï¼šæ‰€æœ‰ä¾èµ–å®‰è£…æˆåŠŸ
- [ ] **æ¶æ„ç†è§£**ï¼šèƒ½è§£é‡ŠDreamæ‰©æ•£æ¨¡å‹çš„å·¥ä½œåŸç†
- [ ] **æ•°æ®å‡†å¤‡**ï¼šS1Kæ•°æ®æˆåŠŸè½¬æ¢ä¸ºDreamæ ¼å¼
- [ ] **è®­ç»ƒå®Œæˆ**ï¼šè‡³å°‘è®­ç»ƒ1-3ä¸ªepochå¹¶ä¿å­˜checkpoint
- [ ] **è¯„ä¼°å®Œæˆ**ï¼šå¯¹æ¯”baseæ¨¡å‹å’Œå¾®è°ƒæ¨¡å‹çš„æ€§èƒ½
- [ ] **æ–‡æ¡£å®Œæ•´**ï¼šè®°å½•æ‰€æœ‰é—®é¢˜ã€è§£å†³æ–¹æ¡ˆå’Œæ€è€ƒè¿‡ç¨‹
- [ ] **Demoå‡†å¤‡**ï¼šèƒ½ç°åœºå±•ç¤ºå¾®è°ƒå‰åçš„æ•ˆæœ

---

## ğŸ“ é‡åˆ°é—®é¢˜æ€ä¹ˆåŠï¼Ÿ

1. **æŸ¥çœ‹å®˜æ–¹æ–‡æ¡£**ï¼šDreamå’ŒS1 çš„GitHub README
2. **æŸ¥çœ‹Issues**ï¼šGitHubä»“åº“çš„Issuesé¡µé¢
3. **ä½¿ç”¨AIåŠ©æ‰‹**ï¼šClaudeã€GPTã€Gemini 2.5 Proç­‰
4. **è”ç³»é¢è¯•å®˜**ï¼šå¾®ä¿¡/ç”µè¯ 17274608033

**è®°ä½**ï¼šè¿‡ç¨‹æ¯”ç»“æœæ›´é‡è¦ï¼è®°å½•æ‚¨çš„æ€è€ƒè¿‡ç¨‹ã€é‡åˆ°çš„é—®é¢˜å’Œè§£å†³æ–¹æ¡ˆã€‚

---

ä¸‹ä¸€æ­¥ï¼šæŸ¥çœ‹ `03_technical_notes.md` äº†è§£å…³é”®æŠ€æœ¯ç»†èŠ‚ã€‚ 