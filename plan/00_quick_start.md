# ðŸš€ å¿«é€Ÿå¼€å§‹æŒ‡å—

> **ç›®æ ‡**: æœ€å¿«ä¸Šæ‰‹ï¼Œç«‹å³å¼€å§‹å®žéªŒï¼

---

## â±ï¸ 5åˆ†é’Ÿå¿«é€Ÿå¼€å§‹

å¦‚æžœä½ æƒ³ç«‹å³å¼€å§‹å®žéªŒï¼Œè·Ÿç€è¿™ä¸ªæ¸…å•èµ°ï¼š

### âœ… æ£€æŸ¥æ¸…å•

```bash
# 1. éªŒè¯çŽ¯å¢ƒ
â–¡ GPUå¯ç”¨ä¸”æ˜¾å­˜â‰¥20GB
â–¡ Python 3.10
â–¡ CUDA 12.1+

# 2. å®‰è£…ä¾èµ– (10åˆ†é’Ÿ)
â–¡ pip install torch==2.5.1 transformers==4.46.2
â–¡ cd resp/Dream && pip install -e .

# 3. æµ‹è¯•æŽ¨ç† (5åˆ†é’Ÿ)
â–¡ è¿è¡Œ test_inference.py ç¡®è®¤æ¨¡åž‹èƒ½åŠ è½½

# 4. å‡†å¤‡æ•°æ® (15åˆ†é’Ÿ)
â–¡ è¿è¡Œ download_s1k.py ä¸‹è½½æ•°æ®
â–¡ è¿è¡Œ prepare_data_for_dream.py è½¬æ¢æ ¼å¼

# 5. å¼€å§‹è®­ç»ƒï¼
â–¡ è¿è¡Œ bash scripts/run_sft_s1k.sh 1 checkpoints/exp1
```

---

## ðŸ“ æŽ¨èçš„ç›®å½•ç»“æž„

åˆ›å»ºä»¥ä¸‹ç›®å½•ç»“æž„ï¼Œè®©å·¥ä½œæ›´æœ‰æ¡ç†ï¼š

```
dream-FineTuned/
â”œâ”€â”€ resp/                      # å·²æœ‰ï¼šä¸‹è½½çš„ä»“åº“
â”‚   â”œâ”€â”€ Dream/
â”‚   â””â”€â”€ s1/
â”œâ”€â”€ plan/                      # å·²æœ‰ï¼šå®žæ–½è®¡åˆ’ï¼ˆå½“å‰æ–‡æ¡£ï¼‰
â”œâ”€â”€ scripts/                   # åˆ›å»ºï¼šè®­ç»ƒè„šæœ¬
â”‚   â”œâ”€â”€ run_sft_s1k.sh
â”‚   â”œâ”€â”€ test_inference.py
â”‚   â””â”€â”€ evaluate.py
â”œâ”€â”€ data/                      # åˆ›å»ºï¼šæ•°æ®ç›®å½•
â”‚   â”œâ”€â”€ s1k/                   # åŽŸå§‹S1Kæ•°æ®
â”‚   â””â”€â”€ s1k_dream_format/      # è½¬æ¢åŽçš„æ•°æ®
â”œâ”€â”€ checkpoints/               # åˆ›å»ºï¼šæ¨¡åž‹æ£€æŸ¥ç‚¹
â”‚   â””â”€â”€ exp1/
â”œâ”€â”€ results/                   # åˆ›å»ºï¼šå®žéªŒç»“æžœ
â”‚   â”œâ”€â”€ logs/
â”‚   â”œâ”€â”€ figures/
â”‚   â””â”€â”€ eval_results.json
â”œâ”€â”€ notebooks/                 # åˆ›å»ºï¼šJupyter notebooks
â”‚   â”œâ”€â”€ explore_data.ipynb
â”‚   â””â”€â”€ analyze_results.ipynb
â””â”€â”€ docs/                      # åˆ›å»ºï¼šæ–‡æ¡£å’Œç¬”è®°
    â”œâ”€â”€ architecture_notes.md
    â”œâ”€â”€ training_log.md
    â””â”€â”€ final_report.md
```

---

## ðŸ› ï¸ ç¬¬ä¸€å¤©å·¥ä½œæµï¼ˆDay 1ï¼‰

### ä¸Šåˆï¼šçŽ¯å¢ƒå‡†å¤‡ (3-4å°æ—¶)

#### æ­¥éª¤1ï¼šåˆ›å»ºå·¥ä½œç›®å½•
```bash
cd /workplace/home/sunzhongao/gpu-usage/dream-FineTuned
mkdir -p scripts data checkpoints results notebooks docs
```

#### æ­¥éª¤2ï¼šå®‰è£…ä¾èµ–
```bash
# åˆ›å»ºcondaçŽ¯å¢ƒ
conda create -n dream-sft python=3.10 -y
conda activate dream-sft

# å®‰è£…æ ¸å¿ƒåº“
pip install torch==2.5.1 --index-url https://download.pytorch.org/whl/cu121
pip install transformers==4.46.2 datasets accelerate
pip install wandb tensorboard tqdm pandas pyarrow

# å®‰è£…Dream
cd resp/Dream
pip install -e .
cd ../..
```

#### æ­¥éª¤3ï¼šæµ‹è¯•çŽ¯å¢ƒ
```bash
# åˆ›å»ºæµ‹è¯•è„šæœ¬
cat > scripts/test_env.py << 'EOF'
import torch
import transformers
print(f"PyTorch: {torch.__version__}")
print(f"Transformers: {transformers.__version__}")
print(f"CUDA: {torch.cuda.is_available()}")
if torch.cuda.is_available():
    print(f"GPU: {torch.cuda.get_device_name(0)}")
    print(f"æ˜¾å­˜: {torch.cuda.get_device_properties(0).total_memory / 1024**3:.1f} GB")
EOF

python scripts/test_env.py
```

### ä¸‹åˆï¼šæ•°æ®å‡†å¤‡ (3-4å°æ—¶)

#### æ­¥éª¤4ï¼šä¸‹è½½S1Kæ•°æ®
```bash
cat > scripts/download_s1k.py << 'EOF'
from datasets import load_dataset
import os

print("ä¸‹è½½S1K-1.1æ•°æ®é›†...")
dataset = load_dataset("simplescaling/s1K-1.1", split="train")
print(f"æ•°æ®é›†å¤§å°: {len(dataset)}")

save_dir = "data/s1k"
os.makedirs(save_dir, exist_ok=True)
dataset.save_to_disk(save_dir)
print(f"ä¿å­˜åˆ°: {save_dir}")

# æ‰“å°æ ·æœ¬
print("\nç¬¬ä¸€ä¸ªæ ·æœ¬:")
print(dataset[0])
EOF

python scripts/download_s1k.py
```

#### æ­¥éª¤5ï¼šè½¬æ¢æ•°æ®æ ¼å¼
```bash
cat > scripts/prepare_data.py << 'EOF'
from datasets import load_from_disk, Dataset
import os

def convert_format(sample):
    question = sample.get('question', sample.get('prompt', ''))
    answer = sample.get('answer', sample.get('response', ''))
    
    prompt = [
        {"role": "system", "content": "You are a helpful AI assistant. Think step by step."},
        {"role": "user", "content": question}
    ]
    
    return {"prompt": prompt, "response": answer}

# åŠ è½½æ•°æ®
dataset = load_from_disk("data/s1k")
print(f"åŽŸå§‹æ•°æ®: {len(dataset)}")

# è½¬æ¢
converted = [convert_format(s) for s in dataset]
converted_dataset = Dataset.from_list(converted)

# åˆ’åˆ†è®­ç»ƒ/éªŒè¯é›†
split = converted_dataset.train_test_split(test_size=0.1, seed=42)

# ä¿å­˜
output_dir = "data/s1k_dream_format"
os.makedirs(output_dir, exist_ok=True)
split['train'].to_parquet(f"{output_dir}/train.parquet")
split['test'].to_parquet(f"{output_dir}/val.parquet")

print(f"è®­ç»ƒé›†: {len(split['train'])}")
print(f"éªŒè¯é›†: {len(split['test'])}")
print(f"ä¿å­˜åˆ°: {output_dir}")
EOF

python scripts/prepare_data.py
```

#### æ­¥éª¤6ï¼šéªŒè¯æ•°æ®
```bash
cat > scripts/validate_data.py << 'EOF'
import pandas as pd
from transformers import AutoTokenizer

train_df = pd.read_parquet("data/s1k_dream_format/train.parquet")
print(f"è®­ç»ƒæ ·æœ¬æ•°: {len(train_df)}")

tokenizer = AutoTokenizer.from_pretrained("Dream-org/Dream-v0-Base-7B", trust_remote_code=True)

lengths = []
for _, row in train_df.iterrows():
    messages = row['prompt'] + [{"role": "assistant", "content": row['response']}]
    tokens = tokenizer.apply_chat_template(messages, tokenize=True)
    lengths.append(len(tokens))

import numpy as np
print(f"Tokené•¿åº¦: min={min(lengths)}, max={max(lengths)}, mean={np.mean(lengths):.0f}")
print(f"è¶…è¿‡2048çš„æ ·æœ¬: {sum(1 for l in lengths if l > 2048)}")
EOF

python scripts/validate_data.py
```

---

## ðŸƒ ç¬¬äºŒå¤©å·¥ä½œæµï¼ˆDay 2-3ï¼‰ï¼šå¼€å§‹è®­ç»ƒ

### åˆ›å»ºè®­ç»ƒè„šæœ¬

```bash
cat > scripts/run_sft_s1k.sh << 'EOF'
#!/bin/bash
set -x

nproc_per_node=${1:-1}
save_path=${2:-checkpoints/s1k-sft}

cd resp/Dream

torchrun --standalone --nnodes=1 --nproc_per_node=$nproc_per_node \
    -m src.trainer.fsdp_sft_trainer \
    diffusion.time_reweighting=cart \
    data.train_files=../../data/s1k_dream_format/train.parquet \
    data.val_files=../../data/s1k_dream_format/val.parquet \
    data.max_length=2048 \
    data.prompt_key=prompt \
    data.response_key=response \
    data.truncation=right \
    optim.lr=2e-6 \
    data.micro_batch_size_per_gpu=2 \
    data.enable_perbatch_cutoff=True \
    data.perbatch_cutoff_type=random_with_input_pad \
    model.partial_pretrain=Dream-org/Dream-v0-Base-7B \
    model.trust_remote_code=True \
    model.enable_gradient_checkpointing=True \
    trainer.default_local_dir=../../$save_path \
    trainer.project_name=dream-s1k-sft \
    trainer.experiment_name=exp-$(date +%Y%m%d-%H%M) \
    trainer.logger=['console','wandb'] \
    trainer.total_epochs=3 \
    trainer.save_freq=500
EOF

chmod +x scripts/run_sft_s1k.sh
```

### å¯åŠ¨è®­ç»ƒ

```bash
# å•GPUè®­ç»ƒï¼ˆ24GBæ˜¾å­˜ï¼‰
bash scripts/run_sft_s1k.sh 1 checkpoints/exp1

# å¦‚æžœæœ‰å¤šGPU
bash scripts/run_sft_s1k.sh 2 checkpoints/exp1
```

### ç›‘æŽ§è®­ç»ƒ

```bash
# ç»ˆç«¯1ï¼šç›‘æŽ§GPU
watch -n 1 nvidia-smi

# ç»ˆç«¯2ï¼šæŸ¥çœ‹æ—¥å¿—
tail -f checkpoints/exp1/train.log

# ç»ˆç«¯3ï¼šå¯åŠ¨tensorboard
tensorboard --logdir checkpoints/exp1
```

---

## ðŸ“Š ç¬¬ä¸‰å¤©å·¥ä½œæµï¼ˆDay 4-5ï¼‰ï¼šè¯„ä¼°ä¸Žå±•ç¤º

### å¿«é€Ÿè¯„ä¼°è„šæœ¬

```bash
cat > scripts/quick_eval.py << 'EOF'
import torch
from transformers import AutoModel, AutoTokenizer

def test_model(model_path, test_prompts):
    model = AutoModel.from_pretrained(model_path, torch_dtype=torch.bfloat16, trust_remote_code=True)
    tokenizer = AutoTokenizer.from_pretrained("Dream-org/Dream-v0-Base-7B", trust_remote_code=True)
    model = model.to("cuda").eval()
    
    for prompt in test_prompts:
        messages = [{"role": "user", "content": prompt}]
        inputs = tokenizer.apply_chat_template(messages, return_tensors="pt", return_dict=True, add_generation_prompt=True)
        input_ids = inputs.input_ids.to("cuda")
        attention_mask = inputs.attention_mask.to("cuda")
        
        output = model.diffusion_generate(
            input_ids, attention_mask=attention_mask,
            max_new_tokens=256, steps=256,
            temperature=0.2, alg="entropy"
        )
        
        response = tokenizer.decode(output.sequences[0][len(input_ids[0]):].tolist())
        print(f"\n{'='*60}")
        print(f"é—®é¢˜: {prompt}")
        print(f"å›žç­”: {response.split(tokenizer.eos_token)[0]}")

# æµ‹è¯•é—®é¢˜
test_prompts = [
    "3x + 7 = 22ï¼Œæ±‚x",
    "å†™ä¸€ä¸ªå¿«é€ŸæŽ’åºçš„Pythonå‡½æ•°",
]

print("=== Baseæ¨¡åž‹ ===")
test_model("Dream-org/Dream-v0-Base-7B", test_prompts)

print("\n\n=== å¾®è°ƒæ¨¡åž‹ ===")
test_model("checkpoints/exp1/checkpoint-final", test_prompts)
EOF

python scripts/quick_eval.py
```

---

## ðŸŽ¯ å¿«é€Ÿæ•…éšœæŽ’é™¤

### é—®é¢˜ï¼šCUDA Out of Memory

```bash
# è§£å†³æ–¹æ¡ˆ1ï¼šå‡å°batch size
# ä¿®æ”¹ run_sft_s1k.sh ä¸­çš„ï¼š
data.micro_batch_size_per_gpu=2  # æ”¹ä¸º1

# è§£å†³æ–¹æ¡ˆ2ï¼šå‡å°åºåˆ—é•¿åº¦
data.max_length=2048  # æ”¹ä¸º1024
```

### é—®é¢˜ï¼šè®­ç»ƒé€Ÿåº¦å¾ˆæ…¢

```bash
# æ£€æŸ¥ï¼š
1. ç¡®è®¤ä½¿ç”¨GPU: nvidia-smi
2. ç¡®è®¤batch sizeä¸æ˜¯å¤ªå°
3. æ£€æŸ¥æ•°æ®åŠ è½½: htop çœ‹CPU
```

### é—®é¢˜ï¼šLossä¸ä¸‹é™

```bash
# æ£€æŸ¥ï¼š
1. å­¦ä¹ çŽ‡æ˜¯å¦åˆç†ï¼ˆ2e-6ï¼‰
2. æ•°æ®æ ¼å¼æ˜¯å¦æ­£ç¡®
3. æ˜¯å¦ä½¿ç”¨äº†Baseæ¨¡åž‹ï¼ˆè€ŒéžInstructï¼‰
```

---

## ðŸ“ æ¯æ—¥å·¥ä½œæ—¥å¿—æ¨¡æ¿

åˆ›å»ºè®­ç»ƒæ—¥å¿—ï¼Œè®°å½•æ¯å¤©çš„è¿›å±•ï¼š

```bash
cat > docs/training_log.md << 'EOF'
# è®­ç»ƒæ—¥å¿—

## Day 1 - 2025-XX-XX

### ä»Šæ—¥ç›®æ ‡
- [ ] çŽ¯å¢ƒé…ç½®
- [ ] æ•°æ®ä¸‹è½½å’Œè½¬æ¢
- [ ] ç†è§£Dreamæž¶æž„

### å®Œæˆæƒ…å†µ
- âœ… 
- â³ 
- âŒ 

### é‡åˆ°çš„é—®é¢˜
1. é—®é¢˜æè¿°
   - åŽŸå› åˆ†æž
   - è§£å†³æ–¹æ¡ˆ

### æŠ€æœ¯ç¬”è®°
- 

### æ˜Žå¤©è®¡åˆ’
- 

---

## Day 2 - 2025-XX-XX

...
EOF
```

---

## ðŸ’¡ çœæ—¶é—´çš„æŠ€å·§

### 1. ä½¿ç”¨å°æ•°æ®é›†å¿«é€ŸéªŒè¯
```python
# åªç”¨10%æ•°æ®è®­ç»ƒ1ä¸ªepochï¼Œå¿«é€ŸéªŒè¯æµç¨‹
split['train'].select(range(100)).to_parquet("data/s1k_dream_format/train_small.parquet")
# ç„¶åŽç”¨train_small.parquetè®­ç»ƒï¼Œ30åˆ†é’Ÿå°±èƒ½çœ‹åˆ°ç»“æžœ
```

### 2. ä½¿ç”¨WandBè‡ªåŠ¨è®°å½•
```bash
wandb login  # åªéœ€ç™»å½•ä¸€æ¬¡
# è®­ç»ƒæ—¥å¿—è‡ªåŠ¨ä¸Šä¼ ï¼Œéšæ—¶éšåœ°æŸ¥çœ‹
```

### 3. ä½¿ç”¨tmuxé¿å…æ–­è¿ž
```bash
# å®‰è£…tmux
sudo apt install tmux

# åˆ›å»ºsession
tmux new -s dream-train

# è¿è¡Œè®­ç»ƒ
bash scripts/run_sft_s1k.sh 1 checkpoints/exp1

# æ–­å¼€ï¼ˆCtrl+B ç„¶åŽ Dï¼‰
# é‡æ–°è¿žæŽ¥ï¼štmux attach -t dream-train
```

### 4. å‡†å¤‡å±•ç¤ºæ¨¡æ¿
```bash
# æå‰å‡†å¤‡å¥½Jupyter notebookåšå¯è§†åŒ–
pip install jupyter matplotlib seaborn
jupyter notebook notebooks/
```

---

## âœ… ç¬¬ä¸€æ¬¡è¿è¡Œæ£€æŸ¥æ¸…å•

åœ¨å¼€å§‹å®Œæ•´è®­ç»ƒå‰ï¼Œç¡®è®¤ï¼š

- [ ] `test_env.py` è¿è¡ŒæˆåŠŸï¼ŒGPUå¯ç”¨
- [ ] S1Kæ•°æ®ä¸‹è½½å®Œæˆï¼Œå¤§å°æ­£ç¡®ï¼ˆ~1000æ ·æœ¬ï¼‰
- [ ] æ•°æ®è½¬æ¢æˆåŠŸï¼Œæ ¼å¼æ­£ç¡®ï¼ˆæœ‰promptå’Œresponseå­—æ®µï¼‰
- [ ] èƒ½ç”¨10æ¡æ•°æ®è·‘é€šè®­ç»ƒæµç¨‹ï¼ˆå“ªæ€•åªè®­ç»ƒå‡ æ­¥ï¼‰
- [ ] wandb/tensorboardèƒ½çœ‹åˆ°æ—¥å¿—
- [ ] ç†è§£äº†Dreamçš„åŸºæœ¬åŽŸç†ï¼ˆæ‰©æ•£è¿‡ç¨‹ï¼‰

**å…¨éƒ¨æ‰“å‹¾åŽï¼Œå†å¼€å§‹å®Œæ•´è®­ç»ƒï¼**

---

## ðŸŽ“ é¢è¯•å‡†å¤‡æ¸…å•

é¢è¯•å‰ä¸€å¤©ï¼Œå‡†å¤‡è¿™äº›ï¼š

- [ ] è®­ç»ƒæ—¥å¿—ï¼ˆè®°å½•é—®é¢˜å’Œè§£å†³æ–¹æ¡ˆï¼‰
- [ ] è®­ç»ƒæ›²çº¿æˆªå›¾ï¼ˆlossä¸‹é™å›¾ï¼‰
- [ ] å¯¹æ¯”æ¡ˆä¾‹ï¼ˆbase vs å¾®è°ƒåŽï¼Œ3-5ä¸ªä¾‹å­ï¼‰
- [ ] PPTæˆ–æ–‡æ¡£ï¼ˆ15åˆ†é’Ÿè®²è§£ï¼‰
- [ ] Demoä»£ç ï¼ˆçŽ°åœºæ¼”ç¤ºï¼‰
- [ ] æž¶æž„ç†è§£ç¬”è®°ï¼ˆèƒ½ç”»å›¾è§£é‡Šï¼‰

---

**çŽ°åœ¨å°±å¼€å§‹å§ï¼** ä»Ž`test_env.py`å¼€å§‹ï¼Œä¸€æ­¥æ­¥æ¥ï¼Œç›¸ä¿¡è‡ªå·±ï¼ðŸ’ª

å¦‚éœ€è¯¦ç»†æ­¥éª¤ï¼ŒæŸ¥çœ‹ï¼š
- `01_overview.md` - é¡¹ç›®æ¦‚è§ˆ
- `02_detailed_steps.md` - è¯¦ç»†æ­¥éª¤
- `03_technical_notes.md` - æŠ€æœ¯è¦ç‚¹ 